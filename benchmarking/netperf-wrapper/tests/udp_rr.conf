## -*- mode: python; coding: utf-8 -*-

import pprint
pp = pprint.PrettyPrinter(indent=4)

AGGREGATOR="timeseries"
TOTAL_LENGTH=LENGTH + 2
NETPERF_CMD="../netperf-2.6.0/src/netperf -fg -4"


AGGREGATOR='summary'
DESCRIPTION='FancyEcho scalability'
DEFAULTS={'PLOT': 'lbest'}
TESTNAME='UDP_RR'
#ITERATIONS=3

PIN_THREADS = True
MAX_CORES = 40 # when using hyperthreads in asiago
MAX_CORES = 20

SERVER_INITIAL_PORT =  888
CLIENT_INITIAL_PORT =  5000

UNIQUE_CLIENTS = list(set(CLIENTS))

def real_test_name():
    """retuns the real test name """
    if (USE_TCP) :
        return "tcp_rr"
    else :
        return "udp_rr"


TESTNAME = real_test_name()

RESULT_LOCATION_BASE = '%s_%s' % (RESULT_LOCATION_BASE2, TESTNAME.strip().replace(' ', ''))
# NOTE: The RESULT_LOCATION_BASE  should have absolute path
# NOTE: This location will be created if not present

assert(SPECIAL_CLIENTS_COUNT == 0)

TOOLS_LOCATION1 = '${HOME}/dragonet/benchmarking/netperf-wrapper/'
TOOLS_LOCATION2 = '${HOME}/git/dragonet/benchmarking/netperf-wrapper/'

include('simple_list_manipulations.py')

server_names=[SERVERS[0] for i in range(0, SERVERS_INSTANCES)]
client_names=expand_client_list(server_names, CLIENTS)
server_if = SERVERS_IF
client_if = CLIENTS_IF
echo_server_name = ECHO_SERVER
psize = PKT_SIZE

# List of flows which this benchmark will be dealing with
FLOWS = create_flows([TARGET], SERVER_INITIAL_PORT,
                client_names, CLIENT_INITIAL_PORT, 1)


server_core_list = gen_core_list(server_names, SERVER_CORES, SERVERS_CORECOUNT, starting_core=SERVER_CORESHIFT)
client_core_list = gen_core_list(client_names, CLIENT_CORES, CLIENTS_CORECOUNT, starting_core=0)


psize = PKT_SIZE
brustsize = BRUST_SIZE
server_onload = SERVER_ONLOAD_CMDLINE
dstat_ignore_initial_x_vals = 4

# FIXME: Change this
custom_header = "pkt=%d, brust=%d srv=%s, onload=%s" % (psize,
            brustsize,
            echo_server_name,
            server_onload,
            )


include('result_parsing_helpers.py')
include('monitors_conf.py') # for tools like dstat, etc

include('echoserver_conf.py')

echo_server_cmds = SRV_CMDS(echo_server_name)

NETPERF_CLIENT_DST_PORT, NETPERF_CLIENT_SRC_PORT, MEMCACHED_PORT_UDP, USING_UDP = get_proto_specific_server_ports()

def create_client(id, clinstance, flowList):
#def create_client(id):

    clName = clinstance[0]
    clid_machine = clinstance[2] # client id within machine


    # Making sure that there is only 1 flow per client
    assert(len(flowList) == 1)

    # Make sure that all flows in flowList go to same server
    #   Not necessary as we only have one flow
    #   But I have kept the code to be compatible with memcached_rr
    assert(len(filter((lambda x:x[2] != clName), flowList)) == 0)

    # Validations on client ports
    #   Not necessary as we only have one flow
    #   But I have kept the code to be compatible with memcached_rr
    clports = sorted(map((lambda x:x[3]), flowList))

    # Make sure that all flows in flowList go to same server
    #   Not necessary as we only have one flow
    #   But I have kept the code to be compatible with memcached_rr
    target = uniqify(map( (lambda x:(x[0], x[1])), flowList ))
    assert(len(target) == 1)
    target_ip = target[0][0]
    target_port = target[0][1]

    if((len(clports) % CLIENT_CORES) != 0):
        print "ERROR: concurrency (%d) is not multiple of client cores (%d)."  % (
                len(clports), CLIENT_CORES)
        print "    This is requirement of memaslap"
        sys.exit(1)
    assert((len(clports) % CLIENT_CORES) == 0)
    assert(len(clports) == (clports[-1] - clports[0] + 1))

    clidstr = "%s:%d-%d" % (clName, clid_machine, id)



    # FIXME: make sure that id is smaller than client_names
    cl = ('client%s' % (clidstr),
         {
          'deployment_host': clName,
          'result_location': '%s_client%s' % (RESULT_LOCATION_BASE, clidstr),
          'tools_location': TOOLS_LOCATION1,
          'is_server': False,
          'TOOLS': o([
            ( 'netperf%s' % (clidstr),
            {
                'command': (''
#                    + 'taskset -c %s ' % toCoreList(client_core_list[id])
                    + '%s %s ' % (
                        NETPERF_CMD,  echo_server_cmds['client_extra_opts'])
                    + '-P 0 -H %s -t %s -l %d ' % (
                        target_ip, TESTNAME, TOTAL_LENGTH )
                    + '-- -r %d -b %d ' % (
                        psize, brustsize)
                    + '-P %d,%d -k all' % (
                        clports[0], target_port)
                    ),

                'runner': 'netperf_sumary',
                'units': 'Gbits/s',
                'wait_for': True,
                'delay': DELAY,
                'is_catastrophic': False,
                'init_cmd': [],
                'out_cmd': [],
                'kill_cmd': ["sudo killall netperf || true"],
            }),

#            ('dstat',
#            {
#                'command': dstatCmd (mname = 'client%d' % (id),
#                    cpus=toCoreList(client_core_list[id]), netdev=client_if[client_names[id]]),
#                'runner': 'dstat_json',
#                'wait_for': True,
#                'delay': DELAY,
#                'is_catastrophic': True,
#                'init_cmd': [],
#                'out_cmd': [],
#                'kill_cmd': [],
#            }),
#

            ]),
          })
    return cl

def create_client_list(client_names, startid=SPECIAL_CLIENTS_COUNT):
    clist = []
    if startid >= len(client_names):
        return clist

    # group flows per client and client-id, so that we can give each client
    # the flows it is responsible for
    flowsGroup = flows_group_by(FLOWS, (lambda x:(x[2], x[4], x[5])))
    client_instances = uniqify(map((lambda x:(x[2], x[4], x[5])), FLOWS))

    print ("client instances: ")
    pp.pprint(client_instances)
    assert(len(client_instances) == len(client_names))

    # We prefer 1 flow per server core
    #assert(len(client_instances) == SERVER_CORES)

    for i in range(startid, len(client_instances)):
        cl_instance = client_instances[i]
        clist.append(create_client(i, cl_instance, flowsGroup[cl_instance]))
    return clist


def create_client_attr(id, mname, attr, label, func):
    if label == None or label == "" :
        label = attr
    label = ("%s_%d_%s" % (label, id, mname))
    return result_attr_wrapper(attr, c="netperf%d" % (id), l=label, func=func)


def create_client_attr_list(client_names,  attr, label=None, func=None,
        startid=SPECIAL_CLIENTS_COUNT):
    clist = []
    if startid >= len(client_names):
        return clist
    for i in range(startid, len(client_names)):
       clist.append(create_client_attr(i, client_names[i], attr, label, func))
    return clist


DATA_SETS = o(
        []
        + create_server_list(server_names)
        + create_client_list(client_names)
        )



def valid_tps(vals) :
    if vals == None or vals == []:
        return 0
    if min(vals) == 0 :
        return 0
    else:
        return sum(vals)

def add_bw(vals) :
    bw = []
    if vals == None or vals == []:
        return 0
    for v in vals:
        if v[-3] == 'M':
            bw.append(float(v[:-3]))
    return sum(bw)

def client_core_list_wrapper():
    if len(client_core_list) > SPECIAL_CLIENTS_COUNT:
        return client_core_list[SPECIAL_CLIENTS_COUNT]
    return []


ATTRIBUTES = o([
    ( 'attrs',
        { 'attrlist' : [
                    meta_attr_wrapper('HWQUEUES'),
                    meta_attr_wrapper('CORES', multiAgg=(lambda x: x['SERVERS_INSTANCES'] * x['SERVER_CORES'])),
                    meta_attr_wrapper('SERVERS_INSTANCES', l="UDP Ports"),
#                    meta_attr_wrapper('SERVERS', agg=len),
                    meta_attr_wrapper('SERVER_CORES', l="Threads/Port"),
                    meta_attr_wrapper('TARGET', agg=(lambda x: target_lookup(x))),

                    meta_attr_wrapper('ECHO_SERVER', l="Server"),
                    meta_attr_wrapper('PKT_SIZE'),
                    meta_attr_wrapper('CLIENTS', agg=(lambda x: len(x) - SPECIAL_CLIENTS_COUNT), type1='invisible'),

                    meta_attr_wrapper('TCONCURRENCY', type1='invisible'),
                    meta_attr_wrapper('CONCURRENCY', type1='invisible'),
#                    result_attr_wrapper("cpu", c="server", l="S CPU", func=get_dstat_attr_cpu, agg=myavg),
                    #result_attr_wrapper("cpu", c="client", l="C CPU", func=get_dstat_attr_cpu, agg=myavg),
                    result_attr_wrapper('TRANSACTION_RATE', c="netperf", l="total TPS", agg=sum),
                    result_attr_wrapper('TRANSACTION_RATE', c="netperf", l="vtotal TPS", agg=valid_tps),
                    #result_attr_wrapper('LOCAL_RECV_THROUGHPUT', c="netperf", l="recv TP", agg=valid_tps),
                    result_attr_wrapper('THROUGHPUT', c="netperf", l="Net_rate", agg=sum),

                    result_attr_wrapper('TRANSACTION_RATE', c="netperf", l="avg TPS", agg=myavg),
                    result_attr_wrapper('TRANSACTION_RATE', c="netperf", l="MIN TPS", agg=min),
                    result_attr_wrapper('MIN_LATENCY', c="netperf", l="get_min", agg=myavg),

                    result_attr_wrapper('MIN_LATENCY', c="netperf", agg=myavg),
                    result_attr_wrapper('RT_LATENCY', c="netperf", agg=myavg),
                    result_attr_wrapper('P50_LATENCY', c="netperf", agg=myavg),
                    result_attr_wrapper('P90_LATENCY', c="netperf", agg=myavg),
                    result_attr_wrapper('P99_LATENCY', c="netperf", agg=myavg),
                    result_attr_wrapper('MAX_LATENCY', c="netperf", agg=myavg),
                    ]

                    + [
                    meta_attr_wrapper('USE_TCP'),
                    #meta_attr_wrapper('SERVER_ONLOAD_CMDLINE', l="Onload"),
                    #result_attr_wrapper('windows size', c="memaslap%d" % (
                    #        SPECIAL_CLIENTS_COUNT), func=get_result_str, agg=find_uniq),

#                   {
#                    'data': get_interrupts,
#                    'args' : {
#                            'mname': 'server-0',
#                            'takeAvg': True},
#                    'label': 'NIC Interrupts'
#                   },

                    meta_attr_wrapper('TITLE', type1='invisible'),
                ]
        }
    )
])



PLOTS = o([
    ('bbox',
     {
      #'description': 'Server: %s, Target: %s, TCP: %s' % (ECHO_SERVER, TARGET, USE_TCP),
      'description': 'TPS, Server: %s, Target: %s %s' % (
            ECHO_SERVER, SERVERS_DRV[SERVERS[0]], TARGET),
      'axis_labels': ["Maximum TPS"],
      'xaxis_label': ["HW queues, Srv Ports, Threads/Port, (simulated clients)"],
      'series': [
                   #meta_attr_wrapper('ECHO_SERVER'),
                   #meta_attr_wrapper('SERVER_ONLOAD_CMDLINE'),
                   #meta_attr_wrapper('TARGET'),
                   #netperf_attr_wrapper ('REQUEST_SIZE'),
                   #netperf_attr_wrapper ('BURST_SIZE'),
                   result_attr_wrapper('TRANSACTION_RATE', l="total TPS"),
                   #result_attr_wrapper('THROUGHPUT', l="Net_rate"),
                   #netperf_attr_wrapper ('MIN_LATENCY'),
                ],
      'type': 'box2',}),

    ('bboxScaleAVG',
     {
      #'description': 'Server: %s, Target: %s, TCP: %s' % (ECHO_SERVER, TARGET, USE_TCP),
      'description': 'TPS, Server: %s, Target: %s %s' % (
            ECHO_SERVER, SERVERS_DRV[SERVERS[0]], TARGET),

      'axis_labels': ["AVG TPS per client"],
      'xaxis_label': ["HW queues, Srv Ports, Threads/Port, (simulated clients)"],
      'groupBy': ['SERVERS_INSTANCES'],
      'series': [
                   result_attr_wrapper('TRANSACTION_RATE', c="netperf", l="avg TPS", agg=myavg, axis=1),
#                   result_attr_wrapper('THROUGHPUT', c="netperf", l="Net_rate", agg=sum, axis=2, color='red'),
                ],
      'type': 'boxscale',}),


    ('bboxScale',
     {
      #'description': 'Server: %s, Target: %s, TCP: %s' % (ECHO_SERVER, TARGET, USE_TCP),
      'description': 'TPS, Server: %s, Target: %s %s' % (
            ECHO_SERVER, SERVERS_DRV[SERVERS[0]], TARGET),

      'dual_axes': True,
      'axis_labels': ["Maximum TPS", 'Bandwidth (Gbps)'],
      'xaxis_label': ["HW queues, Srv Ports, Threads/Port, (simulated clients)"],
      'groupBy': ['SERVERS_INSTANCES'],
      'series': [
                   result_attr_wrapper('TRANSACTION_RATE', c="netperf", l="total TPS", agg=sum, axis=1),
#                   result_attr_wrapper('THROUGHPUT', c="netperf", l="Net_rate", agg=sum, axis=2, color='red'),
                ],
      'type': 'boxscale',}),


    ('bboxNet',
     {
      #'description': 'Server: %s, Target: %s, TCP: %s' % (ECHO_SERVER, TARGET, USE_TCP),
      'description': 'Bandwidth, Server: %s, Target: %s %s' % (
            ECHO_SERVER, SERVERS_DRV[SERVERS[0]], TARGET),
      'axis_labels': ["Bandwidth (Gbps)"],
      'xaxis_label': ["HW queues, Srv Ports, Threads/Port, (simulated clients)"],
      'series': [
                   #meta_attr_wrapper('ECHO_SERVER'),
                   #meta_attr_wrapper('SERVER_ONLOAD_CMDLINE'),
                   meta_attr_wrapper('TARGET'),
                   #netperf_attr_wrapper ('REQUEST_SIZE'),
                   #netperf_attr_wrapper ('BURST_SIZE'),
                   result_attr_wrapper('THROUGHPUT', l="Net_rate"),
                   #netperf_attr_wrapper ('MIN_LATENCY'),
                ],
      'type': 'box2',}),



    ])

#########################################################

